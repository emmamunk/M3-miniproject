---
title: "M3-project"
date: "24 nov 2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
# Introduction

# Problem

# Data desciption

# Data preprocessing 

## Load libaries

We start by cleaning the environment.

```{r, include=FALSE}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
#devtools::install_github("rstudio/keras", force = TRUE)
#library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r}
#Loading packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr,
               readr,
               rmarkdown, 
               tidyverse,
               tidytext,
               dplyr,
               broom,
               keras,
               drat,
               reticulate,
               SciencesPo,
               caret,
               textstem,
               recipes
               )
```

## Data

And now we can download the data from the Github.

```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
```

```{r}
listings = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/listings_detailed.csv")

reviews = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/reviews_detailed.csv")
```

## Data preprocessing

In the variable listings, there's a lof of variables. We have choosen some of them, including id, neighbourhood, room type, minimum nights, number of reviews, reviews per month, calculated host listings count and availability 365.

```{r}
listings = listings %>% select(c(price, zipcode, room_type, accommodates, bathrooms, bedrooms, beds, number_of_reviews, guests_included, review_scores_rating)) %>% filter(room_type %in% c("Entire home/apt", "Private room"))
```


```{r}
listings$price = as.numeric(gsub('[$,]', '', listings$price))
```

```{r}
listings$price = log(listings$price)
```


```{r}
listings$zipcode = as.numeric(gsub('[ ]', '', listings$zipcode))
```

```{r}
listings$room_type = ifelse(listings$room_type == "Entire home/apt", 1, 0)
```

And then we'll remove NA's.

```{r}
listings = na.omit(listings)
```

```{r}
listings
```

# Supervised machine learning

We'll start by splitting the data into test and training.

```{r}
index = createDataPartition(y = listings$price, p = 0.75, list = FALSE)

training = listings[index,] 
test = listings[-index,] 
```



```{r}
reci <- recipe(price ~ ., data = training) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors())

reci = reci %>% step_knnimpute(all_predictors()) 

reci = reci %>% prep(data = training)
```

```{r}
reci
```

```{r}
x_train = bake(reci, new_data = training) %>% select(-price) 
y_train = training %>% pull(price)

x_test = bake(reci, new_data = test) %>% select(-price) 
y_test =  test %>% pull(price)
```

```{r}
ctrl = trainControl(method = "cv", 
                     number = 10)
```

```{r}
fit_lm <- train(x = x_train, 
                y = y_train, 
                trControl = ctrl, 
                method = "glm",
                family = "gaussian")
```

```{r}
fit_lm %>% summary()
```

```{r}
fit_lm
```

```{r}
pred_lm <- fit_lm %>% predict() 
pred_lm %>% head()
```





































############################STOP HER EMMA!!!!!!!!!!!!!!!!!!!!!!####################################
Data preprocessing NLP
Data preprocessing NLP
```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
glimpse(reviews)
```

Tokenize and cleaning
```{r}
reviews_tidy <- reviews %>% 
  unnest_tokens(word, comments) %>% 
  count(id, word, sort = TRUE)

reviews_tidy <- reviews_tidy %>% 
  anti_join(stop_words)
  
#Overview over most used words
reviews_tidy %>% 
  count(word, sort = TRUE)
```


```{r}
reviews_tidy %>% 
  count(word, sort = TRUE) %>% 
  top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n)) + 
  geom_col() +
  xlab(NULL)+
  coord_flip()

```
We see that the individual tokens that are most frequent are words such as "apartment", "stockholm" or  names such as "clean", "close", which describes how the Airbnb home is. 


```{r}
bing_word_counts <- reviews_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word,sentiment, sort = TRUE) %>% 
  ungroup()

bing_word_counts
```
```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(25) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Plotting wordcloud
```{r}
reviews_tidy %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))
```

```{r}
reviews_word <- reviews_tidy %>%
  count(word, sort = TRUE)

total_words <- reviews_word %>% 
  group_by(word) %>% 
  summarize(total = sum(reviews_word$n))

reviews_word <- left_join(reviews_word, total_words, by = "word")

reviews_word
```


## TF-IDF
Up untill now, equal weight have been given to all words, but some are more rare than others. Term frequencyâ€“inverse document frequency or just tf-idf, is a way to analyze how important a word is to a document in a corpus:

$$\text{tf-idf}(t, d) = \text{tf}(t, d) \times \text{idf}(t)$$
Here tf is the term-frequency and idf is the inverse document-frequency, a coefficient which is larger whenever the particular term is found in a lesser number of documents. 


```{r}
reviews_word <- reviews_tidy %>%   count(id, word, sort = TRUE)
total_words <- reviews_word %>% group_by(id) %>%summarize(total = sum(n))
reviews_word <- left_join(reviews_word, total_words)
reviews_word <- reviews_word %>% bind_tf_idf(word, id, n)

reviews_word %>% select(-total) %>% arrange(desc(tf_idf))

```














