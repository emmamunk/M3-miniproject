---
title: "M3-project"
date: "24 nov 2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
# Introduction

# Problem

# Data desciption

# Data preprocessing 

## Load libaries

We start by cleaning the environment.

```{r, include=FALSE}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
#devtools::install_github("rstudio/keras", force = TRUE)
#library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r}
#Loading packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr,
               readr,
               rmarkdown, 
               tidyverse,
               tidytext,
               dplyr,
               broom,
               keras,
               drat,
               reticulate,
               caret,
               textstem,
               recipes,
               MLmetrics,
               e1071,
               GGally
               )
```

## Data

And now we can download the data from the Github.

```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings_detailed.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
```

```{r}
listings = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/listings_detailed.csv")

reviews = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/reviews_detailed.csv")
```

## Data preprocessing

In the variable listings, there's a lof of variables. We have choosen some of them, including id, neighbourhood, room type, minimum nights, number of reviews, reviews per month, calculated host listings count and availability 365.

```{r}
listings = listings %>% select(c(id, price, zipcode, room_type, accommodates, bathrooms, bedrooms, number_of_reviews, guests_included, review_scores_rating)) %>% filter(room_type %in% c("Entire home/apt", "Private room"))
```


```{r}
listings$price = as.numeric(gsub('[$,]', '', listings$price))
```

```{r}
listings$price_intervals = cut(listings$price, c(0, seq(500, 5500, by=500)))
```

```{r}
listings$price_intervals = as.numeric(listings$price_intervals)
```


```{r}
listings$price_intervals[listings$price_intervals == "1"] = "A" 
listings$price_intervals[listings$price_intervals == "2"] = "B"
listings$price_intervals[listings$price_intervals == "3"] = "C"
listings$price_intervals[listings$price_intervals == "4"] = "D"
listings$price_intervals[listings$price_intervals == "5"] = "E"
listings$price_intervals[listings$price_intervals == "6"] = "F"
listings$price_intervals[listings$price_intervals == "7"] = "G"
listings$price_intervals[listings$price_intervals == "8"] = "H"
listings$price_intervals[listings$price_intervals == "9"] = "I"
listings$price_intervals[listings$price_intervals == "10"] = "J"
listings$price_intervals[listings$price_intervals == "11"] = "K"
```



```{r}
listings$zipcode = as.numeric(gsub('[ ]', '', listings$zipcode))
```

```{r}
listings$room_type = ifelse(listings$room_type == "Entire home/apt", 1, 0)
```

And then we'll remove NA's.

```{r}
listings = na.omit(listings)
```

```{r}
ggcorr(listings)
```

```{r}
listings
```




# Supervised machine learning

We'll start by splitting the data into test and training.

```{r}
listings_ml = listings %>% select(c(-price,-id))

index = createDataPartition(y = listings_ml$price_intervals, p = 0.75, list = FALSE)

training = listings_ml[index,] 
test = listings_ml[-index,] 
```



```{r}


reci = recipe(price_intervals ~ ., data = training) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors())

reci = reci %>% step_knnimpute(all_predictors()) 

reci = reci %>% prep(data = training)
```



```{r}
reci
```

```{r}
x_train = bake(reci, new_data = training) %>% select(-price_intervals) 
y_train = training %>% pull(price_intervals) %>% as.factor()

x_test = bake(reci, new_data = test) %>% select(-price_intervals) 
y_test =  test %>% pull(price_intervals) %>% as.factor()
```

```{r}
ctrl <- trainControl(method = "cv", 
                     number = 10, 
                     classProbs = TRUE, 
                     savePredictions = TRUE, 
                     summaryFunction = multiClassSummary, 
                     verboseIter = FALSE,
                     adaptive = list(min = 3, 
                                     alpha = 0.05, 
                                     method = "gls", 
                                     complete = TRUE),
                     search = "random" )

metric <- "Accuracy" 
n_tune = 10 
```

```{r}
fit_dt <- train(x = x_train, 
                y = y_train, 
                trControl = ctrl, 
                metric = metric,
                tuneLength = n_tune,
                method = "rpart") 
```

```{r}
fit_dt
```

```{r}
pred_dt <- predict(fit_dt, newdata = x_test)
```

```{r}
pred_dt %>% head()
```

```{r}
confusionMatrix(pred_dt, y_test)
```


# Mixed Input Model

```{r}
fit.nnet <- train(price ~ ., training, 
              method='nnet', 
              trace = FALSE)

fit.nnet
```

```{r}
plot(fit.nnet)
```
































############################STOP HER EMMA!!!!!!!!!!!!!!!!!!!!!!####################################

Data preprocessing NLP


Loading in the review dataset locally:
After loading we want to take a glimpse on how the dataset is looking. For this we use the function 'glimpse', to give an overview of which variables the dataset contains.
```{r}
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
glimpse(reviews)
```

As the data is very raw and messy, we now want to do some cleaning. We remove everything that isn't normal letters. Which is special characters, numbers and etc. Furthermore we will set all letters from the comments column to lower case.

To clean up the data we are using lemmatization. The purpose of this is to not only analyze the exact word strings in the reviews, as this would include several possible forms of the words used. F. ex. think and thought. Instead we want to merge all possible forms of a word into it's root word. Lemmatization try and do so, by using detailed dictionaries which the algorithm looks trough to link a given word string back to it's root word. This is a more advanced method than stemming and should be beneficial in this report.


```{r}
#Unnest the comments and lemmatize the words
reviews_tidy <- reviews %>% 
  unnest_tokens(word, comments) %>% 
  count(listing_id, word, sort = TRUE) %>% 
  mutate(word = lemmatize_words(word))

#Defining the number of times a word is used the comments
reviews_tidy %>%
  count(word, sort = TRUE)

#Defining own stopwords, that isn't relevant for the analysis
own_stopwords <- tibble(c("2","10","5","7","3","15"),
                        lexicon = "OWN")
#Removing theese homemade stopwords, but also cleaning for general stopwords such as the, as, a and  etc.
reviews_tidy = reviews_tidy %>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")

#Removing all numbers and special characters and removing words who only contain one letter
reviews_tidy = reviews_tidy %>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
    mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 1)
```

After cleaning up the data, we can now look at which words are most represented in the comments. To do this we define a variable called topwords, which will be used to visualize it through a table and wordcloud
```{r}
#Defining the the topwords after cleaning
topwords <- reviews_tidy %>%
  count(word, sort = TRUE)

#Table of topwords
topwords %>%
  top_n(20, n) %>%
  ggplot(aes(x = word %>% fct_reorder(n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Word Counts",
       x = "Frequency",
       y = "Top Words")
```

We see that the individual tokens that are most frequent are words such as "apartment", "stockholm" or names such as "clean", "comfortable", which describes how the Airbnb home is. 

#Sentiment analysis

Sentiment analysis refers to a use of text analysis to extract and identify subjective information, where it analyzises whether the words are positive or negative. In this section, we will be doing two sentiment analysis, first by identifying positive and negative words using the bing lexicon and after this using the afinn lexicon. 

## Bing
We wil start with the Bing lexicon. The Bing lexicon categorizes words in a binary fashion as positive or negative with no weighting. Here, we are using the function get_sentiment to get a specific sentiment lexicon and inner_join to join the lexicon with tokenized data. 

Now we are plotting a word count, grouped by sentiment, showing the 10 most frequent negative and positive words. 

```{r}
sentiment_bing <- reviews_tidy %>% inner_join(get_sentiments("bing"))

sentiment_analysis <- sentiment_bing %>% 
  filter(sentiment %in% c("positive", "negative"))
#Calculating the number of words in each sentiment
word_counts <- sentiment_analysis %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(10, n) %>%
ungroup() %>%
mutate(
word2 = fct_reorder(word, n))

#Plotting the two sentiment with their respective words
ggplot(word_counts, aes(x = word2, y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ sentiment, scales ="free") +
coord_flip() +
labs(title ="Sentiment Word Counts",x ="Words")
```

We see that in general that negative words aren't represented so much as the positive words in the Airbnb comments. Which seems kind odd, then comparing to the expectation beforehand. The reason to the odd distrubtion could be that........................................................................ 





#RNN

```{r}
listings <- listings %>% 
  rename(
    listing_id = id
    )
data <- left_join(reviews, listings, by = "listing_id")

data <- data %>% 
  select(comments, price_intervals)
```

```{r}
index1 <- createDataPartition(y = data$price_intervals, p = 0.75, list = FALSE)

training_price = data[index1,] 
test_price = data[-index1,] 




max_len <- 50
batch_size <- 32
total_epochs <- 15
```

The last step before testing our model is to compile it, and specify what the loss and optimzer should be as what metric to track:
# Optimizer
We are choosing 'rmsprop' which looks back at the general direction of the changes its made previously in order to decide what the next change should be. It's particularly well suited to RNNs.
# Loss
Here, we're going to use "binary crossentropy", which means that we want our model to accurately reflect the probability that an observation will fall into one of two groups.
# Metrics
This argument is optional, but it will tell our model what, in addition to the loss, it should track while it trains. We are going to ask for accuracy so that we can look how accuracy changes as our model trains.

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 89833, output_dim = 32) %>%
  layer_simple_rnn(units = 32) %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

model %>% compile(loss = 'binary_crossentropy', 
                  optimizer = 'RMSprop', 
                  metrics = c('accuracy'))
trained_model <- model %>% fit(
    x = training_price, # sequence we're using for prediction 
    y = test_price, # sequence we're predicting
    batch_size = batch_size, # how many samples to pass to our model at a time
    epochs = total_epochs, # how many times we'll look @ the whole dataset
    validation_split = 0.1) # how much data to hold out for testing as we go along

```




