---
title: "M3-project"
date: "24 nov 2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
# Introduction

# Problem

# Data desciption

# Data preprocessing 

## Load libaries

We start by cleaning the environment.

```{r, include=FALSE}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
#devtools::install_github("rstudio/keras", force = TRUE)
#library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r}
#Loading packages
if (!require("pacman")) install.packages("pacman")
<<<<<<< HEAD
pacman::p_load(knitr, # For knitr to html
               rmarkdown, # For formatting the document
               tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               data.table, # for reading in data ect. 
               magrittr,# For advanced piping (%>% et al.)
               igraph, # For network analysis
               tidygraph, # For tidy-style graph manipulation
               ggraph, # For ggplot2 style graph plotting
               Matrix, # For some matrix functionality
               ggforce, # Awesome plotting
               kableExtra, # Formatting for tables
               car, # recode functions 
               tidytext, # Structure text within tidyverse
               topicmodels, # For topic modelling
               tm, # text mining library
               quanteda, # for LSA (latent semantic analysis)
               uwot, # for UMAP
               dbscan, # for density based clustering
               SnowballC,
               textdata,
               wordcloud, 
               textstem, # for textstemming 
               tidyr,
               widyr,
               reshape2,
               quanteda,
               uwot,
               dbscan,
               plotly,
               rsample,
               glmnet,
               broom,
               yardstick,
               lda, # For LDA-analysis
               topicmodels, # LDA models
               broom
=======
pacman::p_load(knitr,
               readr,
               rmarkdown, 
               tidyverse,
               tidytext,
               dplyr,
               broom,
               keras,
               drat,
               reticulate,
               SciencesPo,
               caret,
               textstem,
               recipes
>>>>>>> parent of 13d39ea... Update Project.Rmd
               )
```

## Data

And now we can download the data from the Github.

```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")

```

```{r}
listings = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/listings_detailed.csv")

reviews = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/reviews_detailed.csv")
```

## Data preprocessing

In the variable listings, there's a lof of variables. We have choosen some of them, including id, neighbourhood, room type, minimum nights, number of reviews, reviews per month, calculated host listings count and availability 365.

```{r}

listings = listings %>% select(c(price, neighbourhood, room_type, property_type, accommodates, bathrooms, bedrooms, beds, bed_type, number_of_reviews, guests_included, review_scores_rating))
```


And then we'll remove NA's.

```{r}
listings = na.omit(listings)
```

After removing NA's, we're down to 6273 observationsa and eight variables. Now, we'll remove every listing, where availability is equal to zero. 

```{r}
listings$price = as.numeric(gsub('[$,]', '', listings$price))
```

```{r}
listings
```

# Supervised machine learning

We'll start by splitting the data into test and training.

```{r}
index = createDataPartition(y = listings$price, p = 0.75, list = FALSE)

training = listings[index,] 
test = listings[-index,] 
```



```{r}
reci <- recipe(price ~ ., data = training) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors())

reci = reci %>% step_knnimpute(all_predictors()) 

reci = reci %>% prep(data = training)
```

```{r}
reci
```

```{r}
x_train = bake(reci, new_data = training) %>% select(-price) 
y_train = training %>% pull(price)

x_test = bake(reci, new_data = test) %>% select(-price) 
y_test =  test %>% pull(price)
```

```{r}
ctrl = trainControl(method = "cv", 
                     number = 10)
```

```{r}
fit_lm <- train(x = x_train, 
                y = y_train, 
                trControl = ctrl, 
                method = "glm",
                family = "gaussian")
```

```{r}
fit_lm %>% summary()
```

```{r}
fit_lm
```

```{r}
tune_glmnet = expand.grid(alpha = seq(0, 1, length = 5), lambda = seq(0, 1, length = 5))
tune_glmnet
```

```{r}
fit_glmnet <- train(x = x_train, 
                    y = y_train, 
                    trControl = ctrl, 
                    method = "glmnet", family = "gaussian", 
                    tuneGrid = tune_glmnet)
```

```{r}
fit_glmnet
```
```{r}
pred_lm = predict(fit_lm, newdata = x_test) 
pred_glmnet <- predict(fit_glmnet, newdata = x_test) 

# RMSE OLS
sqrt(mean( (y_test - predict(fit_lm, newdata = x_test) ) ^ 2))
```

```{r}
sqrt(mean( (y_test - predict(fit_glmnet, newdata = x_test) ) ^ 2))
```


































############################STOP HER EMMA!!!!!!!!!!!!!!!!!!!!!!####################################
Data preprocessing NLP
Data preprocessing NLP
```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
glimpse(reviews)
```

Tokenize and cleaning
```{r}


```


```{r}
reviews_tidy <- reviews %>% 
  unnest_tokens(word, comments) %>% 
  count(id, word, sort = TRUE)


reviews_tidy <- reviews_tidy %>% 
  anti_join(stop_words)
  
#Overview over most used words
reviews_tidy %>% 
  count(word, sort = TRUE)
```


```{r}
reviews_tidy %>% 
  count(word, sort = TRUE) %>% 
  top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word,n)) + 
  geom_col() +
  xlab(NULL)+
  coord_flip()

```
We see that the individual tokens that are most frequent are words such as "apartment", "stockholm" or  names such as "clean", "close", which describes how the Airbnb home is. 


```{r}
bing_word_counts <- reviews_tidy %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word,sentiment, sort = TRUE) %>% 
  ungroup()

bing_word_counts
```
```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(25) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Plotting wordcloud
```{r}
reviews_tidy %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))
```

```{r}
reviews_word <- reviews_tidy %>%
  count(word, sort = TRUE)

total_words <- reviews_word %>% 
  group_by(word) %>% 
  summarize(total = sum(reviews_word$n))

reviews_word <- left_join(reviews_word, total_words, by = "word")

reviews_word
```


## TF-IDF
Up untill now, equal weight have been given to all words, but some are more rare than others. Term frequencyâ€“inverse document frequency or just tf-idf, is a way to analyze how important a word is to a document in a corpus:

$$\text{tf-idf}(t, d) = \text{tf}(t, d) \times \text{idf}(t)$$
Here tf is the term-frequency and idf is the inverse document-frequency, a coefficient which is larger whenever the particular term is found in a lesser number of documents. 


```{r}
reviews_word <- reviews_tidy %>%   count(id, word, sort = TRUE)
total_words <- reviews_word %>% group_by(id) %>%summarize(total = sum(n))
reviews_word <- left_join(reviews_word, total_words)
reviews_word <- reviews_word %>% bind_tf_idf(word, id, n)

reviews_word %>% select(-total) %>% arrange(desc(tf_idf))

```














