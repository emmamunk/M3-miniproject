---
title: "M3-project"
date: "24 nov 2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
# Introduction

# Problem

# Data desciption

# Data preprocessing 

## Load libaries

We start by cleaning the environment.

```{r, include=FALSE}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
#devtools::install_github("rstudio/keras", force = TRUE)
#library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r}
#Loading packages
if (!require("pacman")) install.packages("pacman")

pacman::p_load(knitr, # For knitr to html
               rmarkdown, # For formatting the document
               tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               data.table, # for reading in data ect. 
               magrittr,# For advanced piping (%>% et al.)
               igraph, # For network analysis
               tidygraph, # For tidy-style graph manipulation
               ggraph, # For ggplot2 style graph plotting
               Matrix, # For some matrix functionality
               ggforce, # Awesome plotting
               kableExtra, # Formatting for tables
               car, # recode functions 
               tidytext, # Structure text within tidyverse
               topicmodels, # For topic modelling
               tm, # text mining library
               quanteda, # for LSA (latent semantic analysis)
               uwot, # for UMAP
               dbscan, # for density based clustering
               SnowballC,
               textdata,
               wordcloud, 
               textstem, # for textstemming 
               tidyr,
               widyr,
               reshape2,
               quanteda,
               uwot,
               dbscan,
               plotly,
               rsample,
               glmnet,
               broom,
               yardstick,
               lda, # For LDA-analysis
               topicmodels, # LDA models
               broom,
               caret,
               recipes)
```

## Data

And now we can download the data from the Github.

```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings_detailed.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")

```

```{r}
listings = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/listings_detailed.csv")

reviews = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/reviews_detailed.csv")
```

## Data preprocessing

In the variable listings, there's a lof of variables. We have choosen some of them, including id, neighbourhood, room type, minimum nights, number of reviews, reviews per month, calculated host listings count and availability 365.

```{r}

listings = listings %>% select(c(price, neighbourhood, room_type, property_type, accommodates, bathrooms, bedrooms, beds, bed_type, number_of_reviews, guests_included, review_scores_rating))
```


And then we'll remove NA's.

```{r}
listings = na.omit(listings)
```

After removing NA's, we're down to 6273 observationsa and eight variables. Now, we'll remove every listing, where availability is equal to zero. 

```{r}
listings$price = as.numeric(gsub('[$,]', '', listings$price))
```

```{r}
listings
```

# Supervised machine learning

We'll start by splitting the data into test and training.

```{r}
index = createDataPartition(y = listings$price, p = 0.75, list = FALSE)

training = listings[index,] 
test = listings[-index,] 
```



```{r}
reci <- recipe(price ~ ., data = training) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors())

reci = reci %>% step_knnimpute(all_predictors()) 

reci = reci %>% prep(data = training)
```

```{r}
reci
```

```{r}
x_train = bake(reci, new_data = training) %>% select(-price) 
y_train = training %>% pull(price)

x_test = bake(reci, new_data = test) %>% select(-price) 
y_test =  test %>% pull(price)
```

```{r}
ctrl = trainControl(method = "cv", 
                     number = 10)
```

```{r}
fit_lm <- train(x = x_train, 
                y = y_train, 
                trControl = ctrl, 
                method = "glm",
                family = "gaussian")
```

```{r}
fit_lm %>% summary()
```

```{r}
fit_lm
```

```{r}
tune_glmnet = expand.grid(alpha = seq(0, 1, length = 5), lambda = seq(0, 1, length = 5))
tune_glmnet
```

```{r}
fit_glmnet <- train(x = x_train, 
                    y = y_train, 
                    trControl = ctrl, 
                    method = "glmnet", family = "gaussian", 
                    tuneGrid = tune_glmnet)
```

```{r}
fit_glmnet
```
```{r}
pred_lm = predict(fit_lm, newdata = x_test) 
pred_glmnet <- predict(fit_glmnet, newdata = x_test) 

# RMSE OLS
sqrt(mean( (y_test - predict(fit_lm, newdata = x_test) ) ^ 2))
```

```{r}
sqrt(mean( (y_test - predict(fit_glmnet, newdata = x_test) ) ^ 2))
```


































############################STOP HER EMMA!!!!!!!!!!!!!!!!!!!!!!####################################

Data preprocessing NLP

Loading in the review dataset locally:
After loading we want to take a glimpse on how the dataset is looking. For this we use the function 'glimpse', to give an overview of which variables the dataset contains.
```{r}
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
glimpse(reviews)
```

As the data is very raw and messy, we now want to do some cleaning. We remove everything that isn't normal letters. Which is special characters, numbers and etc. Furthermore we will set all letters from the comments column to lower case.

To clean up the data we are using lemmatization. The purpose of this is to not only analyze the exact word strings in the reviews, as this would include several possible forms of the words used. F. ex. think and thought. Instead we want to merge all possible forms of a word into it's root word. Lemmatization try and do so, by using detailed dictionaries which the algorithm looks trough to link a given word string back to it's root word. This is a more advanced method than stemming and should be beneficial in this report.


```{r}
#Unnest the comments and lemmatize the words
reviews_tidy <- reviews %>% 
  unnest_tokens(word, comments) %>% 
  count(id, word, sort = TRUE) %>% 
  mutate(word = lemmatize_words(word))

#Defining the number of times a word is used the comments
reviews_tidy %>%
  count(word, sort = TRUE)

#Defining own stopwords, that isn't relevant for the analysis
own_stopwords <- tibble(c("2","10","5","7","3","15"),
                        lexicon = "OWN")
#Removing theese homemade stopwords, but also cleaning for general stopwords such as the, as, a and  etc.
reviews_tidy %<>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")

#Removing all numbers and special characters and removing words who only contain one letter
reviews_tidy %<>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
    mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 1)
```

After cleaning up the data, we can now look at which words are most represented in the comments. To do this we define a variable called topwords, which will be used to visualize it through a table and wordcloud
```{r}
#Defining the the topwords after cleaning
topwords <- reviews_tidy %>%
  count(word, sort = TRUE)

#Table of topwords
topwords %>%
  top_n(20, n) %>%
  ggplot(aes(x = word %>% fct_reorder(n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Word Counts",
       x = "Frequency",
       y = "Top Words")
```

We see that the individual tokens that are most frequent are words such as "apartment", "stockholm" or names such as "clean", "comfortable", which describes how the Airbnb home is. 

#Sentiment analysis

Sentiment analysis refers to a use of text analysis to extract and identify subjective information, where it analyzises whether the words are positive or negative. In this section, we will be doing two sentiment analysis, first by identifying positive and negative words using the bing lexicon and after this using the afinn lexicon. 

## Bing
We wil start with the Bing lexicon. The Bing lexicon categorizes words in a binary fashion as positive or negative with no weighting. Here, we are using the function get_sentiment to get a specific sentiment lexicon and inner_join to join the lexicon with tokenized data. 

Now we are plotting a word count, grouped by sentiment, showing the 10 most frequent negative and positive words. 

```{r}
sentiment_bing <- reviews_tidy %>% inner_join(get_sentiments("bing"))

sentiment_analysis <- sentiment_bing %>% 
  filter(sentiment %in% c("positive", "negative"))
#Calculating the number of words in each sentiment
word_counts <- sentiment_analysis %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(10, n) %>%
ungroup() %>%
mutate(
word2 = fct_reorder(word, n))

#Plotting the two sentiment with their respective words
ggplot(word_counts, aes(x = word2, y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ sentiment, scales ="free") +
coord_flip() +
labs(title ="Sentiment Word Counts",x ="Words")
```

We see that in general that negative words aren't represented so much as the positive words in the Airbnb comments. Which seems kind odd, then comparing to the expectation beforehand. The reason to the odd distrubtion could be that........................................................................ 











