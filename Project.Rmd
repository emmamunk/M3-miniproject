---
title: "M3-project"
date: "24 nov 2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
# Introduction

In this project, we'll test six different models to see which one performs better, and we espcially want to analyse whether advanced deep learning models perform better than simple models. We'll analyse Airbnb prices in Stockholm, Sweden, where we'll include numeric, categorical and text data.

# Data desciption

We got our data over Airbnb rooms from https://www.kaggle.com/liubacuzacov/stockholm-sweden-airbnb-listings. They got their data from http://insideairbnb.com/get-the-data.html. Here we load two different files, one for listings details and one for reviews. The one with listings details contains around 7800 observations and 106 variables. The one with reviews has around 119000 observations, and therefore we had to cut it down, but more on that later.

# Data cleaning

Before starting working on the models, we have to do some data cleaning. 

## Load libaries

We start by cleaning the environment.

```{r}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
#devtools::install_github("rstudio/keras", force = TRUE)
#library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r}
#Loading packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr,
               readr,
               rmarkdown, 
               tidyverse,
               tidytext,
               dplyr,
               broom,
               keras,
               drat,
               reticulate,
               caret,
               textstem,
               recipes,
               MLmetrics,
               e1071,
               GGally,
               ranger,
               nnet
               )
```

## Data

And now we can download the data from the Github. We have loaded it in locally. This can be problematic, as it is harder for you to look at the data. However, I was not possibly to load it via Github.

```{r}
listings <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/listings.csv")
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
```



```{r, include=FALSE}
listings = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/listings_detailed.csv")

reviews = read_csv("C:/Users/Emma/OneDrive/Universitetet/stockholm-sweden-airbnb-listings/reviews_detailed.csv")
```

After loading reviews, we had to cut it, as we had problems running some models with this much data. To get a samlpe, which wouldn't be biased, we choose the year 2015, as this year was around 10000 observations. We have to assume that this is representative for all reviews, as the models cannot run if we do it with all years. 

```{r}
reviews = reviews %>% mutate(year = str_detect(date, pattern = "2015")) %>% filter(year == TRUE) %>% select(c(-year))
```


## Data preprocessing

In the variable listings, there's a lof of variables. We have choosen some of them, here id, zipcode, room type, accommodates, bathrooms, bedrooms, number of reviews, guist included and review score rating. We have choosen these, as we think these will help with the understanding of how price is defined. We have to make them all numeric and binary, as we'll have problems further in the assignment if we have categorically variables.

```{r}
listings = listings %>% select(c(id, price, zipcode, room_type, accommodates, bathrooms, bedrooms, number_of_reviews, guests_included, review_scores_rating))
```

### Airbnb price

Our dependent variables is Airbnb price. We have choosen to look at a Classification problem to see how good the models will perform in predicting the price of a room, where price is categorized into six classes.

The first thing, we'll do is making the price numeric and removing the dollar sign in front of it.

```{r}
listings$price = as.numeric(gsub('[$,]', '', listings$price))
```

In this project, we want to look at catogorical variables. Here we set the max price to 5500 dollars and split by 500, so we'll get 11 intervals. We had some outliers with higher prices, but they only had 1 or 2 observations, so we decided to remove them. We removed outliers, where the maximum price is 5500 dollars.

```{r}
listings$price_intervals = cut(listings$price, c(0, seq(500, 5500, by=500)))
```

Adn then we'll make the price intervals numeric.

```{r}
listings$price_intervals = as.numeric(listings$price_intervals)
```

And now we have to change the names, as the models will not run to names, unfortunately. therefore we made two different new variables, one where they are classified as letters, running from A to F. And after that we made six classes running from number 1 to 6. Here everything above 3000 dollars is classified as F and 6, as the classes got smaller and smaller. We still have a very biased data set, where class 2 or B hav around 41 percent of the data, which may cause our models to perform worse compared to if it was equally distributed. First the price interval as letters.

```{r}
#Price intervals as letters
listings$price_intervals_ml[listings$price_intervals == "1"] = "A" 
listings$price_intervals_ml[listings$price_intervals == "2"] = "B"
listings$price_intervals_ml[listings$price_intervals == "3"] = "C"
listings$price_intervals_ml[listings$price_intervals == "4"] = "D"
listings$price_intervals_ml[listings$price_intervals == "5"] = "E"
listings$price_intervals_ml[listings$price_intervals == "6"] = "F"
listings$price_intervals_ml[listings$price_intervals == "7"] = "F"
listings$price_intervals_ml[listings$price_intervals == "8"] = "F"
listings$price_intervals_ml[listings$price_intervals == "9"] = "F"
listings$price_intervals_ml[listings$price_intervals == "10"] = "F"
listings$price_intervals_ml[listings$price_intervals == "11"] = "F"
```

And then as numbers.

```{r}
#Price intervals as numbers
listings$price_intervals_nlp[listings$price_intervals == "1"] = 1 
listings$price_intervals_nlp[listings$price_intervals == "2"] = 2
listings$price_intervals_nlp[listings$price_intervals == "3"] = 3
listings$price_intervals_nlp[listings$price_intervals == "4"] = 4
listings$price_intervals_nlp[listings$price_intervals == "5"] = 5
listings$price_intervals_nlp[listings$price_intervals == "6"] = 6
listings$price_intervals_nlp[listings$price_intervals == "7"] = 6
listings$price_intervals_nlp[listings$price_intervals == "8"] = 6
listings$price_intervals_nlp[listings$price_intervals == "9"] = 6
listings$price_intervals_nlp[listings$price_intervals == "10"] = 6
listings$price_intervals_nlp[listings$price_intervals == "11"] = 6
```

And now we can remove price intervals and price.

```{r}
listings = listings %>% select(c(-price_intervals, -price))
```


### Zipcode

As with price, we'll make the zipcodes numeric.

```{r}
listings$zipcode = as.numeric(gsub('[ ]', '', listings$zipcode))
```
And then we can remove NA's as we will not do more data manipulation for now. Then we can describe the variables more presicely. 

```{r}
listings = na.omit(listings)
```


### Room type

There is all in all four different types of rooms, but shared rooms and hotel rooms were very small combined, so we decided only to look at entire home/apartments or private room combined with shared rooms and hotel rooms, and define the variable as a binary variable. Here entire home/apartment is equal to 1 and private room, hotel room or shared room is equal to 0. And now we can define the binary room type.

```{r}
listings$room_type = ifelse(listings$room_type == "Entire home/apt", 1, 0)
```

And look at the distibution.

```{r}
table(listings$room_type)
```

Here we can see that 4707 of the rooms are entire houses or apartments, while 979 are 

```{r}
listings = na.omit(listings)
```


### Accommodates

Accomoodates describe how many people the place is for.

```{r}
table(listings$accommodates)
```

It goes from 1 to 16, where most have 2-4 people.

### Bathrooms

Bathrooms describe the number of bathrooms available.

```{r}
table(listings$bathrooms)
```

Number of bathrooms goes from 0 to 5, where most have 1 bathrooms.

### Bedrooms

Bedrooms describe the number of bedrooms available.

```{r}
table(listings$bedrooms)
```

Number of bathrooms goes from 0 to 10, where most have 1 bedrooms.

### Number of reviews

Number of reviews describe the number of reviews given.

```{r}
table(listings$number_of_reviews)
```

As seen the number of reviews goes from 1 to 508, where it slowly decreases from 1 and forward.

### Guest included

Guest included describes number of guest included in the price.

```{r}
table(listings$guests_included)
```

Number of bathrooms goes from 1 to 16, where most have 1 guest included.

### Review scores rating

Review scores rating describes a score rating going from 0 to 100, where people could rate the rented rooms/houses.

```{r}
table(listings$review_scores_rating)
```

Number of bathrooms goes from 40 to 100, where 100 is the most common, which is a bit odd. 

### Id

Here the number of identifikation numbers for the Airbnb rooms/apartments. Before any data cleaning and manipulations, there's 7854 identifation numbers. In the end there's 5742 back. 

### Correlation

Now, we want to run a correlation matrix of the different variables to see how much the correlate.

```{r}
ggcorr(listings)
```

Here, it's especially interesting to see what variables price is correlated with. Here, especially accommodates and bedrooms are highly positive correlated and zipcode and number of reviews are slightly negative correlated. And now we can run how the listings look like. Here there's 5686 observatiosna and 13 variables, where some will be removed later on, but this is simple the easiest thing for us to keep them.

```{r}
listings
```


# Supervised machine learning

We'll start by applying supervised machine learning from M1. We really want to see how well simple models perform against more complex models, and therefore it's interesting to start with the decision tree and the random forest to see how good they perform. Our benchmark is 41 percent, as that is what we get if you guess everything in class 2 or class B. 

## Training and test data set

Here, we want to look at the listing and we're choosing not to look at id and price intrvals for nlp. 

```{r}
listings_ml = listings %>% select(c(-id, -price_intervals_nlp))
```

We'll start by splitting the data into test and training. Here, we're doing a 75 percent split for the training data and a 25 percent split for the test data.

```{r}
index = createDataPartition(y = listings_ml$price_intervals_ml, p = 0.75, list = FALSE)

training = listings_ml[index,] 
test = listings_ml[-index,] 
```

Here, we're using the recipes package. It lets you conveniently define a recipe of standard ML preprocessing tasks. Afterwards, we can just can use this recipe to bake our data, meaning performing all the steps in the recipe.

```{r}
reci = recipe(price_intervals_ml ~ ., data = training) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors())

reci = reci %>% prep(data = training)
```

Now we just split again in predictors and outcomes, bake it all, and we are good to go.

```{r}
x_train = bake(reci, new_data = training) %>% select(-price_intervals_ml) 
y_train = training %>% pull(price_intervals_ml) %>% as.factor()

x_test = bake(reci, new_data = test) %>% select(-price_intervals_ml) 
y_test =  test %>% pull(price_intervals_ml) %>% as.factor()
```

We now define a trainControl() object.

```{r}
ctrl = trainControl(method = "cv", 
                     number = 10, 
                     classProbs = TRUE, 
                     savePredictions = TRUE, 
                     summaryFunction = multiClassSummary, 
                     verboseIter = FALSE,
                     adaptive = list(min = 3, 
                                     alpha = 0.05, 
                                     method = "gls", 
                                     complete = TRUE),
                     search = "random" )

metric = "Accuracy" 
n_tune = 10 
```

And then we can train our models

## Decision tree

We start with a decision tree, where we're applying our trainObject().

```{r}
fit_dt = train(x = x_train, 
                y = y_train, 
                trControl = ctrl, 
                metric = metric,
                tuneLength = n_tune,
                method = "rpart") 
```
Adn then printing it for $$n_{tune}=10$$.

```{r}
fit_dt
```

For $n_{tune}=10$ we can see that the accuracy is between 0.47 and 0.507, which isn't good, but is alright. Our goal is to beat 0.41. 

## Random Forest

Let's now run a Random Forest.

```{r}
fit_rf <- train(x = x_train, 
                y = y_train, 
                trControl = ctrl,  
                metric = metric,
                tuneLength = n_tune,
                method = "ranger", 
                importance = "impurity",
                num.trees = 25
                )
```

Adn print it.

```{r}
fit_rf
```
Here the accuracy is between 0.477 and 0.507, which is much like the Decision Tree. 

## Evaluation of Supervised Machine Learning via final out-of-sample prediction

Now we have to test how well is does on the test data. First for the Decision Tree.

```{r}
pred_dt = predict(fit_dt, newdata = x_test)
```

And let's print a confusion matrix.

```{r}
confusionMatrix(pred_dt, y_test)
```

Here the accuracy is slightly better than the training, going up to 0.526. Let's chech the Random Forest.

```{r}
pred_rf = predict(fit_rf, newdata = x_test)
```

Adn again print a confusion matrix.

```{r}
confusionMatrix(pred_rf, y_test)
```

A bit worse, bu much like the Decision Tree. 

# Simple Neural Network

Now we want to check how well a Simple Neural Network will do. This is the simplest neural network, you can do. 

```{r}
fit.nnet = train(price_intervals_ml ~ ., training, 
              method='nnet', 
              trace = FALSE)
```

Adn let's print it. 

```{r}
fit.nnet
```

Overall, this is really performing well.

## Evaluation of Simple Neural Network via final out-of-sample prediction

Let's see how well it does on the test data.

```{r}
pred_nnet = predict(fit.nnet, newdata = x_test)
```

Adn we'll again print a confusion matrix. 

```{r}
confusionMatrix(pred_nnet, y_test)
```

On the test data, it is performing slightly better, but not as good as the Random Forest or Decision Tree. 

































############################STOP HER EMMA!!!!!!!!!!!!!!!!!!!!!!####################################

Data preprocessing NLP


Loading in the review dataset locally:
After loading we want to take a glimpse on how the dataset is looking. For this we use the function 'glimpse', to give an overview of which variables the dataset contains.
```{r}
reviews <- read_csv("C:/Users/Michael Thomsen/Desktop/Stockholm/reviews_detailed.csv")
glimpse(reviews)
```

As the data is very raw and messy, we now want to do some cleaning. We remove everything that isn't normal letters. Which is special characters, numbers and etc. Furthermore we will set all letters from the comments column to lower case.

To clean up the data we are using lemmatization. The purpose of this is to not only analyze the exact word strings in the reviews, as this would include several possible forms of the words used. F. ex. think and thought. Instead we want to merge all possible forms of a word into it's root word. Lemmatization try and do so, by using detailed dictionaries which the algorithm looks trough to link a given word string back to it's root word. This is a more advanced method than stemming and should be beneficial in this report.


```{r}
#Unnest the comments and lemmatize the words
reviews_tidy <- reviews %>% 
  unnest_tokens(word, comments) %>% 
  count(listing_id, word, sort = TRUE) %>% 
  mutate(word = lemmatize_words(word))

#Defining the number of times a word is used the comments
reviews_tidy %>%
  count(word, sort = TRUE)

#Defining own stopwords, that isn't relevant for the analysis
own_stopwords <- tibble(c("2","10","5","7","3","15"),
                        lexicon = "OWN")
#Removing theese homemade stopwords, but also cleaning for general stopwords such as the, as, a and  etc.
reviews_tidy = reviews_tidy %>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")

#Removing all numbers and special characters and removing words who only contain one letter
reviews_tidy = reviews_tidy %>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
    mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 1)
```

After cleaning up the data, we can now look at which words are most represented in the comments. To do this we define a variable called topwords, which will be used to visualize it through a table and wordcloud
```{r}
#Defining the the topwords after cleaning
topwords <- reviews_tidy %>%
  count(word, sort = TRUE)

#Table of topwords
topwords %>%
  top_n(20, n) %>%
  ggplot(aes(x = word %>% fct_reorder(n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Word Counts",
       x = "Frequency",
       y = "Top Words")
```

We see that the individual tokens that are most frequent are words such as "apartment", "stockholm" or names such as "clean", "comfortable", which describes how the Airbnb home is. 

#Sentiment analysis

Sentiment analysis refers to a use of text analysis to extract and identify subjective information, where it analyzises whether the words are positive or negative. In this section, we will be doing two sentiment analysis, first by identifying positive and negative words using the bing lexicon and after this using the afinn lexicon. 

## Bing
We wil start with the Bing lexicon. The Bing lexicon categorizes words in a binary fashion as positive or negative with no weighting. Here, we are using the function get_sentiment to get a specific sentiment lexicon and inner_join to join the lexicon with tokenized data. 

Now we are plotting a word count, grouped by sentiment, showing the 10 most frequent negative and positive words. 

```{r}
sentiment_bing <- reviews_tidy %>% inner_join(get_sentiments("bing"))

sentiment_analysis <- sentiment_bing %>% 
  filter(sentiment %in% c("positive", "negative"))
#Calculating the number of words in each sentiment
word_counts <- sentiment_analysis %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(10, n) %>%
ungroup() %>%
mutate(
word2 = fct_reorder(word, n))

#Plotting the two sentiment with their respective words
ggplot(word_counts, aes(x = word2, y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ sentiment, scales ="free") +
coord_flip() +
labs(title ="Sentiment Word Counts",x ="Words")
```

We see that in general that negative words aren't represented so much as the positive words in the Airbnb comments. Which seems kind odd, then comparing to the expectation beforehand. The reason to the odd distrubtion could be that........................................................................ 





#Creating a simple Recurrent Neural Network model


```{r}
listings1 <- listings %>% rename(listing_id = id)
data <- left_join(reviews, listings1, by = "listing_id")

data <- data %>% 
  select(comments, price_intervals_nlp, listing_id) %>%
  na.omit()



```

```{r}
index1 <- createDataPartition(y = data$price_intervals_nlp, p = 0.75, list = FALSE)

training <- data[index1,] 
test <- data[-index1,] 

# Price intervals (y)
training_price = training$price_intervals_nlp
test_price = test$price_intervals_nlp

# Comments intervals (x)
training_comments = training$comments
test_comments = test$comments
```

```{r}
max_features = 2000
tokenizer = text_tokenizer(num_words = max_features,
  filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
  lower = TRUE, split = " ", char_level = FALSE)
```

Here I update the tokenizer internal vocabulary based on tweets.

```{r}
keras::fit_text_tokenizer(tokenizer, training_comments)
```

```{r}
tweet_tokenizer = keras::texts_to_sequences(tokenizer, training_comments)
```

And then we can look at the first six tweets and how the words now take numeric values.

```{r}
keras::fit_text_tokenizer(tokenizer, test_comments)
```

```{r}
tweet_tokenizer2 = keras::texts_to_sequences(tokenizer, test_comments)
```


```{r}
#Unnest the comments and lemmatize the words
training_tidy <- training_comments %>% 
  unnest_tokens(word, comments) %>% 
  count(listing_id, word, sort = TRUE) %>% 
  mutate(word = lemmatize_words(word))

#Defining own stopwords, that isn't relevant for the analysis
own_stopwords <- tibble(c("2","10","5","7","3","15"),
                        lexicon = "OWN")
#Removing theese homemade stopwords, but also cleaning for general stopwords such as the, as, a and  etc.
training_tidy <- training_tidy %>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")

#Removing all numbers and special characters and removing words who only contain one letter
training_tidy <- training_tidy %>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
    mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 1)


#Unnest the comments and lemmatize the words
test_tidy <- test_comments %>% 
  unnest_tokens(word, comments) %>% 
  count(listing_id, word, sort = TRUE) %>% 
  mutate(word = lemmatize_words(word))

#Removing theese homemade stopwords, but also cleaning for general stopwords such as the, as, a and  etc.
test_tidy <- test_tidy %>%
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")

#Removing all numbers and special characters and removing words who only contain one letter
test_tidy <- test_tidy %>%
  mutate(word = word %>% str_remove_all("[^[:alnum:]]") ) %>%
    mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 1)
```


```{r}
max_features <- 10000
#Training
tokenizer_training <- text_tokenizer(num_words = max_features)


tokenizer_training %>% 
  fit_text_tokenizer(training_tidy$word)

tokenizer_training$document_count


text_seqs_training <- texts_to_sequences(tokenizer_training, data$comments)

text_seqs_training %>%
  head()

#Testing
tokenizer_test <- text_tokenizer(num_words = max_features)


tokenizer_test %>% 
  fit_text_tokenizer(test_tidy$word)

tokenizer_test$document_count


text_seqs_test <- texts_to_sequences(tokenizer_test, data$comments)

text_seqs_test %>%
  head()
```

```{r}
x_train_nlp <- pad_sequences(tweet_tokenizer, maxlen = 100)
x_test <- pad_sequences(tweet_tokenizer2, maxlen = 100)
```


The last step before testing our model is to compile it, and specify what the loss and optimzer should be as what metric to track:
# Optimizer
We are choosing 'rmsprop' which looks back at the general direction of the changes its made previously in order to decide what the next change should be. It's particularly well suited to RNNs.
# Loss
Here, we're going to use "binary crossentropy", which means that we want our model to accurately reflect the probability that an observation will fall into one of two groups.
# Metrics
This argument is optional, but it will tell our model what, in addition to the loss, it should track while it trains. We are going to ask for accuracy so that we can look how accuracy changes as our model trains.

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 2000, output_dim = 100) %>%
  layer_simple_rnn(units = 100, return_sequences = TRUE) %>% 
  layer_dropout(0.2) %>%
  layer_simple_rnn(units = 50, return_sequences = FALSE) %>%
  layer_dense(units = 7, activation = "softmax")

summary(model)

model %>% compile(loss = 'sparse_categorical_crossentropy', 
                  optimizer = 'RMSprop', 
                  metrics = c('accuracy'))

trained_model <- model %>% fit(
    x = x_train_nlp, # sequence we're using for prediction 
    y = training_price, # sequence we're predicting
    batch_size = 1024, # how many samples to pass to our model at a time
    epochs = 10, # how many times we'll look @ the whole dataset
    validation_split = 0.10) # how much data to hold out for testing as we go along

```

```{r}
dim(listings_ml)
```


# Mixed Input Model

We wanted to experiment with a mixed input model, which is a model that draws from different data sources. Here, we're using  numeric and categorical data from listings and text data from reviews. We want to make a model that combines those two. 

```{r}
listings_mim = listings %>% rename(listing_id = id)
data = left_join(reviews, listings_mim, by = "listing_id")

data_mim = data %>% 
  select(c(-listing_id, -id, -date, -reviewer_name, -reviewer_id, -price_intervals_ml)) %>%
  na.omit()
```

```{r}
index2 <- createDataPartition(y = data_mim$price_intervals_nlp, p = 0.75, list = FALSE)

training <- data[index2,] 
test <- data[-index2,] 

training = na.omit(training)
test = na.omit(test)

# Price intervals (y)
training_price = training$price_intervals_nlp
test_price = test$price_intervals_nlp

# Comments intervals (x)
training_comments = training$comments
test_comments = test$comments
```

```{r}
reci_mim = recipe(price_intervals_ml ~ zipcode + room_type + accommodates + bathrooms + bedrooms + number_of_reviews + guests_included + review_scores_rating, data = training)

reci_mim = reci %>% prep(data = training)
```

Now we just split again in predictors and outcomes, bake it all, and we are good to go.

```{r}
x_train = bake(reci, new_data = training) %>% select(-price_intervals_ml) 
y_train = training %>% pull(price_intervals_ml) %>% as.factor()

x_test = bake(reci, new_data = test) %>% select(-price_intervals_ml) 
y_test =  test %>% pull(price_intervals_ml) %>% as.factor()
```

We now define a trainControl() object.

```{r}
ctrl = trainControl(method = "cv", 
                     number = 10, 
                     classProbs = TRUE, 
                     savePredictions = TRUE, 
                     summaryFunction = multiClassSummary, 
                     verboseIter = FALSE,
                     adaptive = list(min = 3, 
                                     alpha = 0.05, 
                                     method = "gls", 
                                     complete = TRUE),
                     search = "random" )

metric = "Accuracy" 
n_tune = 10 
```


```{r}
max_features = 2000
tokenizer = text_tokenizer(num_words = max_features,
  filters = "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
  lower = TRUE, split = " ", char_level = FALSE)
```

Here I update the tokenizer internal vocabulary based on tweets.

```{r}
keras::fit_text_tokenizer(tokenizer, training_comments)
```

```{r}
tweet_tokenizer = keras::texts_to_sequences(tokenizer, training_comments)
```

And then we can look at the first six tweets and how the words now take numeric values.

```{r}
keras::fit_text_tokenizer(tokenizer, test_comments)
```

```{r}
tweet_tokenizer2 = keras::texts_to_sequences(tokenizer, test_comments)
```


```{r}
x_train_nlp = pad_sequences(tweet_tokenizer, maxlen = 100)
x_test = pad_sequences(tweet_tokenizer2, maxlen = 100)
```

```{r}
dim(x_train_nlp)
```


```{r}
# Numeric and categorical model
bi_input = layer_input(shape = c(8))

#bi_output = layer_input %>%
  #layer()
  #layer_dense(units = 64, activation = 'relu')

# Text model
main_input = layer_input(shape = c(100))



text_out = main_input %>% 
  layer_embedding(input_dim = 2000, output_dim = 512, input_length = 100) %>% 
  layer_lstm(units = 32)

# Mix layer
main_output = layer_concatenate(c(text_out, bi_input)) %>%  
  layer_dropout(0.5) %>%
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dropout(0.5) %>%
  layer_dense(units = 7, activation = 'softmax')

# Model
model = keras_model(
  inputs = c(main_input, bi_input), 
  outputs = c(main_output)
)
```
 
```{r}
summary(model)
```

```{r}
model %>% compile(loss = 'sparse_categorical_crossentropy', 
                  optimizer = 'RMSprop', 
                  metrics = c('accuracy'))
```

```{r}
x = as.matrix(x_train)
```


```{r}
model %>% fit(
  x = list(x_train_nlp, x),
  y = training_price,
  epochs = 10,
  batch_size = 256,
  validation_split = 0.25
)
```









